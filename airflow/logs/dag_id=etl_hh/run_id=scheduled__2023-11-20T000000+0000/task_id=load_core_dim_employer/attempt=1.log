[2023-11-21 15:09:18,361] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 15:09:18,395] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 15:09:18,396] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 15:09:18,398] {taskinstance.py:1377} INFO - Starting attempt 1 of 2
[2023-11-21 15:09:18,399] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 15:09:18,449] {taskinstance.py:1397} INFO - Executing <Task(PostgresOperator): load_core_dim_employer> on 2023-11-20 00:00:00+00:00
[2023-11-21 15:09:18,459] {standard_task_runner.py:52} INFO - Started process 1182 to run task
[2023-11-21 15:09:18,465] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_hh', 'load_core_dim_employer', 'scheduled__2023-11-20T00:00:00+00:00', '--job-id', '255', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpydecui2x', '--error-file', '/tmp/tmpee0vgzfr']
[2023-11-21 15:09:18,467] {standard_task_runner.py:80} INFO - Job 255: Subtask load_core_dim_employer
[2023-11-21 15:09:18,676] {task_command.py:371} INFO - Running <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [running]> on host f44f09cebae7
[2023-11-21 15:09:18,950] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_hh
AIRFLOW_CTX_TASK_ID=load_core_dim_employer
AIRFLOW_CTX_EXECUTION_DATE=2023-11-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-11-20T00:00:00+00:00
[2023-11-21 15:09:18,974] {base.py:68} INFO - Using connection ID 'my_db_conn' for task execution.
[2023-11-21 15:09:19,010] {dbapi.py:231} INFO - Running statement: 
        MERGE INTO core.dim_employer AS trg
        USING(
            SELECT DISTINCT 
                employer_id,
                employer_name
            FROM staging.vacancy) AS src
        ON trg.employer_id = src.employer_id
        WHEN NOT MATCHED
        THEN INSERT(
            employer_id,
            name)
        VALUES(
            src.employer_id,
            src.employer_name)
        WHEN MATCHED
                AND (COALESCE(trg.name, '') <> COALESCE(src.employer_name, '')
                OR trg.employer_id <> src.employer_id)
        THEN UPDATE
        SET 
            employer_id = src.employer_id, 
            name = src.employer_name, parameters: None
[2023-11-21 15:09:19,117] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=etl_hh, task_id=load_core_dim_employer, execution_date=20231120T000000, start_date=20231121T150918, end_date=20231121T150919
[2023-11-21 15:09:19,241] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-11-21 15:09:19,418] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-11-21 15:27:06,459] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 15:27:06,490] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 15:27:06,491] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 15:27:06,492] {taskinstance.py:1377} INFO - Starting attempt 1 of 2
[2023-11-21 15:27:06,493] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 15:27:06,531] {taskinstance.py:1397} INFO - Executing <Task(PostgresOperator): load_core_dim_employer> on 2023-11-20 00:00:00+00:00
[2023-11-21 15:27:06,540] {standard_task_runner.py:52} INFO - Started process 289 to run task
[2023-11-21 15:27:06,545] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_hh', 'load_core_dim_employer', 'scheduled__2023-11-20T00:00:00+00:00', '--job-id', '272', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpmgr260up', '--error-file', '/tmp/tmp44wbfqgw']
[2023-11-21 15:27:06,546] {standard_task_runner.py:80} INFO - Job 272: Subtask load_core_dim_employer
[2023-11-21 15:27:06,671] {task_command.py:371} INFO - Running <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [running]> on host b47ed23e9d21
[2023-11-21 15:27:06,845] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_hh
AIRFLOW_CTX_TASK_ID=load_core_dim_employer
AIRFLOW_CTX_EXECUTION_DATE=2023-11-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-11-20T00:00:00+00:00
[2023-11-21 15:27:06,862] {base.py:68} INFO - Using connection ID 'my_db_conn' for task execution.
[2023-11-21 15:27:06,905] {dbapi.py:231} INFO - Running statement: 
        MERGE INTO core.dim_employer AS trg
        USING(
            SELECT DISTINCT 
                employer_id,
                employer_name
            FROM staging.vacancy) AS src
        ON trg.employer_id = src.employer_id
        WHEN NOT MATCHED
        THEN INSERT(
            employer_id,
            name)
        VALUES(
            src.employer_id,
            src.employer_name)
        WHEN MATCHED
                AND (COALESCE(trg.name, '') <> COALESCE(src.employer_name, '')
                OR trg.employer_id <> src.employer_id)
        THEN UPDATE
        SET 
            employer_id = src.employer_id, 
            name = src.employer_name, parameters: None
[2023-11-21 15:27:06,962] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=etl_hh, task_id=load_core_dim_employer, execution_date=20231120T000000, start_date=20231121T152706, end_date=20231121T152706
[2023-11-21 15:27:07,039] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-11-21 15:27:07,231] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-11-21 15:44:10,288] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 15:44:10,303] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 15:44:10,304] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 15:44:10,304] {taskinstance.py:1377} INFO - Starting attempt 1 of 2
[2023-11-21 15:44:10,305] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 15:44:10,323] {taskinstance.py:1397} INFO - Executing <Task(PostgresOperator): load_core_dim_employer> on 2023-11-20 00:00:00+00:00
[2023-11-21 15:44:10,329] {standard_task_runner.py:52} INFO - Started process 1050 to run task
[2023-11-21 15:44:10,333] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_hh', 'load_core_dim_employer', 'scheduled__2023-11-20T00:00:00+00:00', '--job-id', '286', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmp5rm103ip', '--error-file', '/tmp/tmp7lidqdjr']
[2023-11-21 15:44:10,334] {standard_task_runner.py:80} INFO - Job 286: Subtask load_core_dim_employer
[2023-11-21 15:44:10,413] {task_command.py:371} INFO - Running <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [running]> on host b47ed23e9d21
[2023-11-21 15:44:10,512] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_hh
AIRFLOW_CTX_TASK_ID=load_core_dim_employer
AIRFLOW_CTX_EXECUTION_DATE=2023-11-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-11-20T00:00:00+00:00
[2023-11-21 15:44:10,524] {base.py:68} INFO - Using connection ID 'my_db_conn' for task execution.
[2023-11-21 15:44:10,545] {dbapi.py:231} INFO - Running statement: 
        MERGE INTO core.dim_employer AS trg
        USING(
            SELECT DISTINCT 
                employer_id,
                employer_name
            FROM staging.vacancy) AS src
        ON trg.employer_id = src.employer_id
        WHEN NOT MATCHED
        THEN INSERT(
            employer_id,
            name)
        VALUES(
            src.employer_id,
            src.employer_name)
        WHEN MATCHED
                AND (COALESCE(trg.name, '') <> COALESCE(src.employer_name, '')
                OR trg.employer_id <> src.employer_id)
        THEN UPDATE
        SET 
            employer_id = src.employer_id, 
            name = src.employer_name, parameters: None
[2023-11-21 15:44:10,577] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=etl_hh, task_id=load_core_dim_employer, execution_date=20231120T000000, start_date=20231121T154410, end_date=20231121T154410
[2023-11-21 15:44:10,625] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-11-21 15:44:10,699] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-11-21 16:03:01,885] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 16:03:01,900] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 16:03:01,901] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 16:03:01,901] {taskinstance.py:1377} INFO - Starting attempt 1 of 2
[2023-11-21 16:03:01,902] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 16:03:01,926] {taskinstance.py:1397} INFO - Executing <Task(PostgresOperator): load_core_dim_employer> on 2023-11-20 00:00:00+00:00
[2023-11-21 16:03:01,933] {standard_task_runner.py:52} INFO - Started process 1916 to run task
[2023-11-21 16:03:01,936] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_hh', 'load_core_dim_employer', 'scheduled__2023-11-20T00:00:00+00:00', '--job-id', '304', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpcz72vsmw', '--error-file', '/tmp/tmp9efys6dy']
[2023-11-21 16:03:01,937] {standard_task_runner.py:80} INFO - Job 304: Subtask load_core_dim_employer
[2023-11-21 16:03:02,023] {task_command.py:371} INFO - Running <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [running]> on host b47ed23e9d21
[2023-11-21 16:03:02,139] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_hh
AIRFLOW_CTX_TASK_ID=load_core_dim_employer
AIRFLOW_CTX_EXECUTION_DATE=2023-11-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-11-20T00:00:00+00:00
[2023-11-21 16:03:02,153] {base.py:68} INFO - Using connection ID 'my_db_conn' for task execution.
[2023-11-21 16:03:02,177] {dbapi.py:231} INFO - Running statement: 
        MERGE INTO core.dim_employer AS trg
        USING(
            SELECT DISTINCT 
                employer_id,
                employer_name
            FROM staging.vacancy) AS src
        ON trg.employer_id = src.employer_id
        WHEN NOT MATCHED
        THEN INSERT(
            employer_id,
            name)
        VALUES(
            src.employer_id,
            src.employer_name)
        WHEN MATCHED
                AND (COALESCE(trg.name, '') <> COALESCE(src.employer_name, '')
                OR trg.employer_id <> src.employer_id)
        THEN UPDATE
        SET 
            employer_id = src.employer_id, 
            name = src.employer_name, parameters: None
[2023-11-21 16:03:02,209] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=etl_hh, task_id=load_core_dim_employer, execution_date=20231120T000000, start_date=20231121T160301, end_date=20231121T160302
[2023-11-21 16:03:02,269] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-11-21 16:03:02,329] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-11-21 16:05:02,273] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 16:05:02,292] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 16:05:02,293] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 16:05:02,294] {taskinstance.py:1377} INFO - Starting attempt 1 of 2
[2023-11-21 16:05:02,294] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 16:05:02,317] {taskinstance.py:1397} INFO - Executing <Task(PostgresOperator): load_core_dim_employer> on 2023-11-20 00:00:00+00:00
[2023-11-21 16:05:02,325] {standard_task_runner.py:52} INFO - Started process 2015 to run task
[2023-11-21 16:05:02,329] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_hh', 'load_core_dim_employer', 'scheduled__2023-11-20T00:00:00+00:00', '--job-id', '309', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpbzcqhzyl', '--error-file', '/tmp/tmpr447d5ll']
[2023-11-21 16:05:02,329] {standard_task_runner.py:80} INFO - Job 309: Subtask load_core_dim_employer
[2023-11-21 16:05:02,427] {task_command.py:371} INFO - Running <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [running]> on host b47ed23e9d21
[2023-11-21 16:05:02,582] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_hh
AIRFLOW_CTX_TASK_ID=load_core_dim_employer
AIRFLOW_CTX_EXECUTION_DATE=2023-11-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-11-20T00:00:00+00:00
[2023-11-21 16:05:02,596] {base.py:68} INFO - Using connection ID 'my_db_conn' for task execution.
[2023-11-21 16:05:02,621] {dbapi.py:231} INFO - Running statement: 
        MERGE INTO core.dim_employer AS trg
        USING(
            SELECT DISTINCT 
                employer_id,
                employer_name
            FROM staging.vacancy) AS src
        ON trg.employer_id = src.employer_id
        WHEN NOT MATCHED
        THEN INSERT(
            employer_id,
            name)
        VALUES(
            src.employer_id,
            src.employer_name)
        WHEN MATCHED
                AND (COALESCE(trg.name, '') <> COALESCE(src.employer_name, '')
                OR trg.employer_id <> src.employer_id)
        THEN UPDATE
        SET 
            employer_id = src.employer_id, 
            name = src.employer_name, parameters: None
[2023-11-21 16:05:02,659] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=etl_hh, task_id=load_core_dim_employer, execution_date=20231120T000000, start_date=20231121T160502, end_date=20231121T160502
[2023-11-21 16:05:02,702] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-11-21 16:05:02,784] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-11-21 16:14:22,481] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 16:14:22,506] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [queued]>
[2023-11-21 16:14:22,507] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 16:14:22,507] {taskinstance.py:1377} INFO - Starting attempt 1 of 2
[2023-11-21 16:14:22,508] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2023-11-21 16:14:22,533] {taskinstance.py:1397} INFO - Executing <Task(PostgresOperator): load_core_dim_employer> on 2023-11-20 00:00:00+00:00
[2023-11-21 16:14:22,543] {standard_task_runner.py:52} INFO - Started process 2453 to run task
[2023-11-21 16:14:22,547] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'etl_hh', 'load_core_dim_employer', 'scheduled__2023-11-20T00:00:00+00:00', '--job-id', '325', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpzr658wvc', '--error-file', '/tmp/tmp1j7o3fk2']
[2023-11-21 16:14:22,548] {standard_task_runner.py:80} INFO - Job 325: Subtask load_core_dim_employer
[2023-11-21 16:14:22,643] {task_command.py:371} INFO - Running <TaskInstance: etl_hh.load_core_dim_employer scheduled__2023-11-20T00:00:00+00:00 [running]> on host b47ed23e9d21
[2023-11-21 16:14:22,759] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=etl_hh
AIRFLOW_CTX_TASK_ID=load_core_dim_employer
AIRFLOW_CTX_EXECUTION_DATE=2023-11-20T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-11-20T00:00:00+00:00
[2023-11-21 16:14:22,774] {base.py:68} INFO - Using connection ID 'my_db_conn' for task execution.
[2023-11-21 16:14:22,798] {dbapi.py:231} INFO - Running statement: 
        MERGE INTO core.dim_employer AS trg
        USING(
            SELECT DISTINCT 
                employer_id,
                employer_name
            FROM staging.vacancy) AS src
        ON trg.employer_id = src.employer_id
        WHEN NOT MATCHED
        THEN INSERT(
            employer_id,
            name)
        VALUES(
            src.employer_id,
            src.employer_name)
        WHEN MATCHED
                AND (COALESCE(trg.name, '') <> COALESCE(src.employer_name, '')
                OR trg.employer_id <> src.employer_id)
        THEN UPDATE
        SET 
            employer_id = src.employer_id, 
            name = src.employer_name, parameters: None
[2023-11-21 16:14:22,833] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=etl_hh, task_id=load_core_dim_employer, execution_date=20231120T000000, start_date=20231121T161422, end_date=20231121T161422
[2023-11-21 16:14:22,880] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-11-21 16:14:22,956] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
